{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "WATERMARK = False\n",
    "GATECH_USERNAME = 'DO NOT STEAL'\n",
    "TERM = 'SPRING 2019'\n",
    "\n",
    "def watermark(p):\n",
    "    if not WATERMARK:\n",
    "        return p\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for i in range(1, 11):\n",
    "        p.text(0.95, 0.95 - (i * (1.0/10)), '{} {}'.format(GATECH_USERNAME, TERM), transform=ax.transAxes,\n",
    "               fontsize=32, color='gray',\n",
    "               ha='right', va='bottom', alpha=0.2)\n",
    "    return p\n",
    "\n",
    "def plot_policy_map(title, policy, map_desc, color_map, direction_map):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, xlim=(0, policy.shape[1]), ylim=(0, policy.shape[0]))\n",
    "    font_size = 'x-large'\n",
    "    if policy.shape[1] >= 16:\n",
    "        font_size = 'x-small'\n",
    "    plt.title(title)\n",
    "    for i in range(policy.shape[0]):\n",
    "        for j in range(policy.shape[1]):\n",
    "            y = policy.shape[0] - i - 1\n",
    "            x = j\n",
    "            p = plt.Rectangle([x, y], 1, 1)\n",
    "            p.set_facecolor(color_map[map_desc[i, j]])\n",
    "            ax.add_patch(p)\n",
    "\n",
    "            text = ax.text(x+0.5, y+0.5, direction_map[policy[i, j]], weight='bold', size=font_size,\n",
    "                           horizontalalignment='center', verticalalignment='center', color='w')\n",
    "            text.set_path_effects([path_effects.Stroke(linewidth=2, foreground='black'),\n",
    "                                   path_effects.Normal()])\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.xlim((0, policy.shape[1]))\n",
    "    plt.ylim((0, policy.shape[0]))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return watermark(plt)\n",
    "\n",
    "\n",
    "def plot_value_map(title, v, map_desc, color_map):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, xlim=(0, v.shape[1]), ylim=(0, v.shape[0]))\n",
    "    font_size = 'x-large'\n",
    "    if v.shape[1] >= 16:\n",
    "        font_size = 'x-small'\n",
    "\n",
    "    v_min = np.min(v)\n",
    "    v_max = np.max(v)\n",
    "    bins = np.linspace(v_min, v_max, 100)\n",
    "    v_red = np.digitize(v, bins)/100.0\n",
    "#     for i in range(v.shape[0]):\n",
    "#         for j in range(v.shape[1]):\n",
    "#             value = np.round(v[i, j], 2)\n",
    "#             if len(str(value)) > 4:\n",
    "#                 font_size = 'x-small'\n",
    "\n",
    "    plt.title(title)\n",
    "    for i in range(v.shape[0]):\n",
    "        for j in range(v.shape[1]):\n",
    "            y = v.shape[0] - i - 1\n",
    "            x = j\n",
    "            p = plt.Rectangle([x, y], 1, 1)\n",
    "            p.set_facecolor(color_map[map_desc[i, j]])\n",
    "            ax.add_patch(p)\n",
    "\n",
    "            value = np.round(v[i, j], 2)\n",
    "\n",
    "            red = v_red[i, j]\n",
    "            text2 = ax.text(x+0.5, y+0.5, value, size=font_size,\n",
    "                            horizontalalignment='center', verticalalignment='center', color=(1.0, 1.0-red, 1.0-red))\n",
    "            text2.set_path_effects([path_effects.Stroke(linewidth=1, foreground='black'),\n",
    "                                   path_effects.Normal()])\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.xlim((0, v.shape[1]))\n",
    "    plt.ylim((0, v.shape[0]))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return watermark(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentStats(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.policies = list()\n",
    "        self.vs = list()\n",
    "        self.steps = list()\n",
    "        self.step_times = list()\n",
    "        self.rewards = list()\n",
    "        self.deltas = list()\n",
    "        self.converged_values = list()\n",
    "        self.elapsed_time = 0\n",
    "        self.optimal_policy = None\n",
    "\n",
    "    def add(self, policy, v, step, step_time, reward, delta, converged):\n",
    "        self.policies.append(policy)\n",
    "        self.vs.append(v)\n",
    "        self.steps.append(step)\n",
    "        self.step_times.append(step_time)\n",
    "        self.rewards.append(reward)\n",
    "        self.deltas.append(delta)\n",
    "        self.converged_values.append(converged)\n",
    "\n",
    "    def to_csv(self, file_name):\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(\"steps,time,reward,delta,converged\\n\")\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerows(zip(self.steps, self.step_times, self.rewards, self.deltas, self.converged_values))\n",
    "\n",
    "    def pickle_results(self, file_name_base, map_shape, step_size=1, only_last=False):\n",
    "        if only_last:\n",
    "            policy = np.reshape(np.argmax(self.policies[-1], axis=1), map_shape)\n",
    "            v = self.vs[-1].reshape(map_shape)\n",
    "            file_name = file_name_base.format('Last')\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump({'policy': policy, 'v': v}, f)\n",
    "        else:\n",
    "            l = len(self.policies)\n",
    "            if step_size == 1 and l > 20:\n",
    "                step_size = math.floor(l/20.0)\n",
    "            for i, policy in enumerate(self.policies):\n",
    "                if i % step_size == 0 or i == l-1:\n",
    "                    v = self.vs[i].reshape(map_shape)\n",
    "                    file_name = file_name_base.format(i)\n",
    "                    if i == l-1:\n",
    "                        file_name = file_name_base.format('Last')\n",
    "                    with open(file_name, 'wb') as f:\n",
    "                        pickle.dump({'policy': np.reshape(np.argmax(policy, axis=1), map_shape), 'v': v}, f)\n",
    "\n",
    "    def plot_policies_on_map(self, file_name_base, map_desc, color_map, direction_map, experiment, step_preamble,\n",
    "                             details, step_size=1, only_last=False):\n",
    "        if only_last:\n",
    "            policy = self.policies[-1]\n",
    "            v = self.vs[-1]\n",
    "\n",
    "            policy_file_name = file_name_base.format('Policy', 'Last')\n",
    "            value_file_name = file_name_base.format('Value', 'Last')\n",
    "            title = '{}: {} - {} {}'.format(details.env_readable_name, experiment, 'Last', step_preamble)\n",
    "\n",
    "            p = plot_policy_map(title, policy, map_desc, color_map, direction_map)\n",
    "            p.savefig(policy_file_name, format='png', dpi=150)\n",
    "            p.close()\n",
    "\n",
    "            p = plot_value_map(title, v, map_desc, color_map)\n",
    "            p.savefig(value_file_name, format='png', dpi=150)\n",
    "            p.close()\n",
    "        else:\n",
    "            l = len(self.policies)\n",
    "            if step_size == 1 and l > 20:\n",
    "                step_size = math.floor(l/20.0)\n",
    "            for i, policy in enumerate(self.policies):\n",
    "                if i % step_size == 0 or i == l-1:\n",
    "                    policy = np.reshape(np.argmax(policy, axis=1), map_desc.shape)\n",
    "                    v = self.vs[i].reshape(map_desc.shape)\n",
    "\n",
    "                    file_name = file_name_base.format('Policy', i)\n",
    "                    value_file_name = file_name_base.format('Value', i)\n",
    "                    if i == l-1:\n",
    "                        file_name = file_name_base.format('Policy', 'Last')\n",
    "                        value_file_name = file_name_base.format('Value', 'Last')\n",
    "\n",
    "                    title = '{}: {} - {} {}'.format(details.env_readable_name, experiment, step_preamble, i)\n",
    "\n",
    "                    p = plot_policy_map(title, policy, map_desc, color_map, direction_map)\n",
    "                    p.savefig(file_name, format='png', dpi=150)\n",
    "                    p.close()\n",
    "\n",
    "                    p = plot_value_map(title, v, map_desc, color_map)\n",
    "                    p.savefig(value_file_name, format='png', dpi=150)\n",
    "                    p.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'policies: {}, vs: {}, steps: {}, step_times: {}, deltas: {}, converged_values: {}'.format(\n",
    "            self.policies,\n",
    "            self.vs,\n",
    "            self.steps,\n",
    "            self.step_times,\n",
    "            self.deltas,\n",
    "            self.converged_values\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    def __init__(self, size, reward, start=None, terminal_1=None, terminal_2=None, blocked=None):\n",
    "        self.board = np.zeros(size)\n",
    "        self.rows = size[0]\n",
    "        self.cols = size[1]\n",
    "        self.reward = reward\n",
    "        self.actions = ['north', 'east', 'south', 'west']\n",
    "        self.model = {'north': [((-1, 0), 0.8), ((0, 1), 0.1), ((0, -1), 0.1)],\n",
    "                      'east': [((0, 1), 0.8), ((-1, 0), 0.1), ((1, 0), 0.1)],\n",
    "                      'south': [((1, 0), 0.8), ((0, 1), 0.1), ((0, -1), 0.1)],\n",
    "                      'west': [((0, -1), 0.8), ((-1, 0), 0.1), ((1, 0), 0.1)]}\n",
    "        if not start:\n",
    "            self.start = (np.random.choice(self.rows), 0)\n",
    "        else:\n",
    "            self.start = start\n",
    "            \n",
    "        if not terminal_1:\n",
    "            self.terminal_1 = (np.random.choice(self.rows), self.cols-1)\n",
    "        else:\n",
    "            self.terminal_1 = terminal_1\n",
    "            \n",
    "        if not terminal_2:\n",
    "            self.terminal_2 = (np.random.choice(self.rows), np.random.choice(range(self.cols//2, self.cols-1)))\n",
    "        else:\n",
    "            self.terminal_2 = terminal_2\n",
    "            \n",
    "        if not blocked:\n",
    "            blocked = (np.random.choice(self.rows), np.random.choice(range(1, self.cols//2)))\n",
    "            while blocked == self.start or blocked == self.terminal_1 or blocked == self.terminal_2:\n",
    "                blocked = (np.random.choice(self.rows), np.random.choice(self.cols))\n",
    "            self.blocked = blocked\n",
    "        else:\n",
    "            self.blocked = blocked\n",
    "            \n",
    "        self.desc = np.asarray([['W' for j in range(self.cols)] for i in range(self.rows)], dtype='c')\n",
    "        self.desc[self.start] = 'S'\n",
    "        self.desc[self.terminal_1] = 'G'\n",
    "        self.desc[self.terminal_2] = 'R'\n",
    "        self.desc[self.blocked] = 'B'\n",
    "        \n",
    "    def colors(self):\n",
    "        return {\n",
    "            b'S': 'yellow',\n",
    "            b'W': 'lightslategray',\n",
    "            b'B': 'black',\n",
    "            b'G': 'green',\n",
    "            b'R': 'red'\n",
    "        }\n",
    "\n",
    "    def directions(self):\n",
    "        return {\n",
    "            0: '\\u2191',\n",
    "            1: '\\u2192',\n",
    "            2: '\\u2193',\n",
    "            3: '\\u2190'\n",
    "        }\n",
    "        \n",
    "    def rewards(self, state):\n",
    "        if state == self.terminal_1:\n",
    "            return 1\n",
    "        elif state == self.terminal_2:\n",
    "            return -1\n",
    "        elif state == self.blocked:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.reward\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        \n",
    "        if state == self.terminal_1 or state == self.terminal_2:\n",
    "            return 'terminal', 0, True\n",
    "        else:\n",
    "            transition, _ = self.model[action][np.random.choice(range(3), p=[0.8, 0.1, 0.1])]\n",
    "            i, j = state\n",
    "            delta_i, delta_j = transition\n",
    "            next_state = np.clip(i + delta_i, 0, self.rows - 1), np.clip(j + delta_j, 0, self.cols - 1)\n",
    "\n",
    "            if next_state == self.blocked:\n",
    "                next_state = state\n",
    "\n",
    "            reward = self.rewards(next_state)\n",
    "\n",
    "            return next_state, reward, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \n",
    "    def __init__(self, env, gamma=0.9, theta=1e-5):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.delta = np.ones(self.env.board.shape)\n",
    "        self.value = np.zeros(self.env.board.shape)\n",
    "        self.policy = np.zeros(self.env.board.shape)\n",
    "        self.steps = 0\n",
    "        self.step_times = []\n",
    "        \n",
    "    def step(self):\n",
    "        start = time.time()\n",
    "        reward = 0\n",
    "        new_value = np.zeros(self.value.shape)\n",
    "        for i in range(self.env.rows):\n",
    "            for j in range(self.env.cols):\n",
    "                vals = []\n",
    "                state = i, j\n",
    "                if state == self.env.terminal_1 or state == self.env.terminal_2 or state == self.env.blocked:\n",
    "                    new_value[state] = self.env.rewards(state)\n",
    "                else:\n",
    "                    for action in self.env.actions:\n",
    "                        val = 0\n",
    "                        for transition, prob in self.env.model[action]:\n",
    "                            delta_i, delta_j = transition\n",
    "                            next_state = np.clip(i + delta_i, 0, self.env.rows - 1), np.clip(j + delta_j, 0, self.env.cols - 1)\n",
    "                            if next_state == self.env.blocked:\n",
    "                                next_state = state                            \n",
    "                            val += prob * (self.env.rewards(state) + self.gamma * self.value[next_state])\n",
    "                        vals.append(val)\n",
    "                    new_value[i, j] = max(vals)\n",
    "                    self.policy[i, j] = np.argmax(vals)\n",
    "                    reward += max(vals)\n",
    "        self.delta = np.abs(self.value - new_value)\n",
    "        max_delta = np.max(self.delta)\n",
    "        self.value = new_value\n",
    "        self.step_times.append(time.time() - start)\n",
    "        \n",
    "#         self.policy = np.zeros((self.env.rows, self.env.cols))#, len(self.env.actions)))\n",
    "#         for i in range(self.env.rows):\n",
    "#             for j in range(self.env.cols):\n",
    "#                 vals = []\n",
    "#                 state = i, j\n",
    "#                 if state == self.env.terminal_1 or state == self.env.terminal_2 or state == self.env.blocked:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     for action in self.env.actions:\n",
    "#                         val = 0\n",
    "#                         for transition, prob in self.env.model[action]:\n",
    "#                             delta_i, delta_j = transition\n",
    "#                             next_state = np.clip(i + delta_i, 0, self.env.rows - 1), np.clip(j + delta_j, 0, self.env.cols - 1)\n",
    "#                             if next_state == blocked:\n",
    "#                                 next_state = state                            \n",
    "#                             val += prob * (self.env.rewards(state) + self.gamma * self.value[next_state])\n",
    "#                         vals.append(val)\n",
    "#                     self.policy[i, j] = np.argmax(vals)\n",
    "                    \n",
    "        self.steps += 1\n",
    "        \n",
    "        return self.policy, self.value, self.steps, self.step_times[-1], reward, max_delta, self.has_converged()\n",
    "                    \n",
    "    def has_converged(self):\n",
    "        return np.all(self.delta < self.theta)\n",
    "    \n",
    "    def run_to_convergence(self):\n",
    "        while not self.has_converged():\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \n",
    "    def __init__(self, env, gamma=0.9, theta=1e-5, max_conv_steps=3):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.delta = np.ones(self.env.board.shape)\n",
    "        self.policy_delta = 1\n",
    "        self.value = np.zeros(self.env.board.shape)\n",
    "        self.policy = np.random.choice(len(self.env.actions), (self.env.rows, self.env.cols))\n",
    "        self.policy[self.env.start] = 0\n",
    "        self.policy[self.env.terminal_1] = 0\n",
    "        self.policy[self.env.terminal_2] = 0\n",
    "        self.policy[self.env.blocked] = 0\n",
    "        self.steps = 0\n",
    "        self.step_times = []\n",
    "        self.conv_steps = 0\n",
    "        self.max_conv_steps = max_conv_steps\n",
    "        \n",
    "    def evaluate_policy(self):\n",
    "        converged = False\n",
    "        while not converged:\n",
    "            new_value = np.zeros(self.value.shape)\n",
    "            for i in range(self.env.rows):\n",
    "                for j in range(self.env.cols):\n",
    "                    state = i, j\n",
    "                    if state == self.env.terminal_1 or state == self.env.terminal_2 or state == self.env.blocked:\n",
    "                        new_value[i, j] = self.env.rewards(state)\n",
    "                    else:\n",
    "                        val = 0\n",
    "                        for transition, prob in self.env.model[self.env.actions[int(self.policy[state])]]:\n",
    "                            delta_i, delta_j = transition\n",
    "                            next_state = np.clip(i + delta_i, 0, self.env.rows - 1), np.clip(j + delta_j, 0, self.env.cols - 1)\n",
    "                            if next_state == self.env.blocked:\n",
    "                                next_state = state                            \n",
    "                            val += prob * (self.env.rewards(state) + self.gamma * self.value[next_state])\n",
    "                        new_value[i, j] = val\n",
    "            delta = np.abs(self.value - new_value)\n",
    "            converged = np.all(delta < self.theta)\n",
    "            self.value = new_value\n",
    "        \n",
    "    def step(self):\n",
    "        start = time.time()\n",
    "        self.evaluate_policy()\n",
    "        reward = 0\n",
    "        new_value = np.zeros(self.value.shape)\n",
    "        new_policy = np.zeros(self.value.shape)\n",
    "        for i in range(self.env.rows):\n",
    "            for j in range(self.env.cols):\n",
    "                vals = []\n",
    "                state = i, j\n",
    "                if state == self.env.terminal_1 or state == self.env.terminal_2 or state == self.env.blocked:\n",
    "                    continue\n",
    "                else:\n",
    "                    for action in self.env.actions:\n",
    "                        val = 0\n",
    "                        for transition, prob in self.env.model[action]:\n",
    "                            delta_i, delta_j = transition\n",
    "                            next_state = np.clip(i + delta_i, 0, self.env.rows - 1), np.clip(j + delta_j, 0, self.env.cols - 1)\n",
    "                            if next_state == self.env.blocked:\n",
    "                                next_state = state                            \n",
    "                            val += prob * (self.env.rewards(state) + self.gamma * self.value[next_state])\n",
    "                        vals.append(val)\n",
    "                    new_policy[state] = np.argmax(vals)\n",
    "                    new_value[state] = max(vals)\n",
    "        \n",
    "        self.policy_delta = np.abs(self.policy - new_policy)\n",
    "        self.delta = np.abs(self.value - new_value)\n",
    "        max_delta = np.max(self.delta)\n",
    "        self.policy = new_policy\n",
    "        self.steps += 1\n",
    "        self.step_times.append(time.time() - start)\n",
    "        \n",
    "#         print(self.policy_delta)\n",
    "#         print(self.delta)\n",
    "#         print(self.delta < self.theta)\n",
    "#         print(np.all(self.delta < self.theta))\n",
    "        \n",
    "        if np.all(self.policy_delta == 0):\n",
    "            self.conv_steps += 1\n",
    "        else:\n",
    "            self.conv_steps = 0\n",
    "        \n",
    "        return self.policy, self.value, self.steps, self.step_times[-1], reward, max_delta, self.has_converged()\n",
    "         \n",
    "    def has_converged(self):\n",
    "        return self.conv_steps >= self.max_conv_steps or np.all(self.delta < self.theta)\n",
    "    \n",
    "    def run_to_convergence(self):\n",
    "        while not self.has_converged():\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \n",
    "    def __init__(self, env, gamma=0.9, theta=1e-5, epsilon=0.99999, epsilon_decay=0.99995, alpha=0.5, \n",
    "                 steps_per_episode=100, max_steps=100000, strategy='epsilon-greedy'):\n",
    "        \n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.alpha = alpha\n",
    "        self.steps = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.step_times = []\n",
    "        self.conv_steps = 0\n",
    "        self.max_conv_steps = 10\n",
    "        self.strategy = strategy\n",
    "        if strategy == 'epsilon-greedy':\n",
    "            self.q_function = {(i, j): [0]*len(self.env.actions) for i in range(self.env.rows) for j in range(self.env.cols)}\n",
    "            self.q_function['terminal'] = [0]*len(self.env.actions)\n",
    "        else:\n",
    "            self.q_function = {(i, j): [1]*len(self.env.actions) for i in range(self.env.rows) for j in range(self.env.cols)}\n",
    "            self.q_function['terminal'] = [1]*len(self.env.actions)\n",
    "        \n",
    "    def step(self):\n",
    "        start = time.time()\n",
    "        total_reward = 0\n",
    "        episode_steps = 0\n",
    "        state = self.env.start\n",
    "        self.delta = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if self.strategy == 'epsilon-greedy':\n",
    "                if np.random.random() < 1 - self.epsilon:\n",
    "                    action = self.env.actions[np.argmax(self.q_function[state])]\n",
    "                else:\n",
    "                    action = np.random.choice(self.env.actions)\n",
    "            else:\n",
    "                action = self.env.actions[np.argmax(self.q_function[state])]\n",
    "                \n",
    "            action_idx = self.env.actions.index(action)\n",
    "            next_state, reward, done = self.env.step(state, action)\n",
    "            total_reward += reward\n",
    "            td_error = reward + self.gamma * np.max(self.q_function[next_state]) - self.q_function[state][action_idx]\n",
    "            self.q_function[state][action_idx] += (self.alpha * td_error)\n",
    "            self.delta = max(self.delta, td_error)\n",
    "            state = next_state\n",
    "            episode_steps += 1\n",
    "\n",
    "        self.steps += 1\n",
    "        self.step_times.append(time.time() - start)\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        avg_reward = total_reward / episode_steps\n",
    "        self.get_value_policy()\n",
    "        \n",
    "        if self.delta < self.theta:\n",
    "            self.conv_steps += 1\n",
    "        else:\n",
    "            self.conv_steps = 0\n",
    "\n",
    "        return self.policy, self.value, self.steps, self.step_times[-1], avg_reward, self.delta, self.has_converged()\n",
    "    \n",
    "    def get_value_policy(self):\n",
    "        self.value = np.zeros(self.env.board.shape)\n",
    "        self.policy = np.zeros(self.env.board.shape)\n",
    "        for i in range(self.value.shape[0]):\n",
    "            for j in range(self.value.shape[1]):\n",
    "                self.value[i, j] = np.max(self.q_function[(i, j)])\n",
    "                self.policy[i, j] = np.argmax(self.q_function[(i, j)])\n",
    "        \n",
    "    def has_converged(self):\n",
    "        return self.conv_steps >= self.max_conv_steps or \\\n",
    "               self.steps >= self.max_steps# or self.epsilon < 0.001\n",
    "    \n",
    "    def run_to_convergence(self):\n",
    "        while not self.has_converged():\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solver_and_collect(solver):\n",
    "    stats = ExperimentStats()\n",
    "\n",
    "    t = time.time()\n",
    "    step_count = 0\n",
    "    optimal_policy = None\n",
    "    best_reward = float('-inf')\n",
    "\n",
    "    while not solver.has_converged():# and step_count < MAX_STEP_COUNT:\n",
    "        policy, v, steps, step_time, reward, delta, converged = solver.step()\n",
    "        # print('{} {}'.format(reward, best_reward))\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            optimal_policy = policy\n",
    "\n",
    "        stats.add(policy, v, steps, step_time, reward, delta, converged)\n",
    "        step_count += 1\n",
    "\n",
    "    stats.elapsed_time = time.time() - t\n",
    "    stats.optimal_policy = stats.policies[-1]  # optimal_policy\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentDetails(object):\n",
    "    def __init__(self, env, env_name, env_readable_name, threads, seed):\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.env_readable_name = env_readable_name\n",
    "        self.threads = threads\n",
    "        self.seed = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve MDPs with VI, PI, QL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIRECTORY = './output'\n",
    "rewards = [-2, -0.5, -0.05, -0.005]\n",
    "gammas = [0.1, 0.5, 0.9]\n",
    "alphas = [0.25, 0.5, 0.75]\n",
    "sm_start, sm_terminal_1, sm_terminal_2, sm_blocked = (3, 0), (0, 3), (1, 3), (2, 1)\n",
    "lg_start, lg_terminal_1, lg_terminal_2, lg_blocked = (15, 0), (0, 15), (4, 14), (7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running VI with reward: -2, gamma: 0.1\n",
      "Running VI with reward: -2, gamma: 0.1\n",
      "Running VI with reward: -2, gamma: 0.5\n",
      "Running VI with reward: -2, gamma: 0.5\n",
      "Running VI with reward: -2, gamma: 0.9\n",
      "Running VI with reward: -2, gamma: 0.9\n",
      "Running VI with reward: -0.5, gamma: 0.1\n",
      "Running VI with reward: -0.5, gamma: 0.1\n",
      "Running VI with reward: -0.5, gamma: 0.5\n",
      "Running VI with reward: -0.5, gamma: 0.5\n",
      "Running VI with reward: -0.5, gamma: 0.9\n",
      "Running VI with reward: -0.5, gamma: 0.9\n",
      "Running VI with reward: -0.05, gamma: 0.1\n",
      "Running VI with reward: -0.05, gamma: 0.1\n",
      "Running VI with reward: -0.05, gamma: 0.5\n",
      "Running VI with reward: -0.05, gamma: 0.5\n",
      "Running VI with reward: -0.05, gamma: 0.9\n",
      "Running VI with reward: -0.05, gamma: 0.9\n",
      "Running VI with reward: -0.005, gamma: 0.1\n",
      "Running VI with reward: -0.005, gamma: 0.1\n",
      "Running VI with reward: -0.005, gamma: 0.5\n",
      "Running VI with reward: -0.005, gamma: 0.5\n",
      "Running VI with reward: -0.005, gamma: 0.9\n",
      "Running VI with reward: -0.005, gamma: 0.9\n"
     ]
    }
   ],
   "source": [
    "for reward in rewards:\n",
    "    \n",
    "    sm_gridworld = GridWorld((4, 4), reward, sm_start, sm_terminal_1, sm_terminal_2, sm_blocked)\n",
    "    lg_gridworld = GridWorld((16, 16), reward, lg_start, lg_terminal_1, lg_terminal_2, lg_blocked)\n",
    "    \n",
    "    envs = [sm_gridworld, lg_gridworld]\n",
    "    #envs = [sm_gridworld]\n",
    "    #envs = [lg_gridworld]\n",
    "    \n",
    "    small_details = ExperimentDetails(sm_gridworld, 'sm_gridworld', 'Gridworld (4x4)', 1, 1)\n",
    "    large_details = ExperimentDetails(lg_gridworld, 'lg_gridworld', 'Gridworld (16x16)', 1, 1)\n",
    "    \n",
    "    details = [small_details, large_details]\n",
    "    #details = [small_details]\n",
    "    #details = [large_details]\n",
    "    \n",
    "    for gamma in gammas:\n",
    "        \n",
    "        for i, env in enumerate(envs):\n",
    "            \n",
    "            print('Running VI with reward: {}, gamma: {}'.format(reward, gamma))\n",
    "            \n",
    "            vi = ValueIteration(env, gamma=gamma)\n",
    "            vi_stats = run_solver_and_collect(vi)\n",
    "            vi_stats.plot_policies_on_map('{}/images/VI/{}_{}_{}_{}.png'.format(OUTPUT_DIRECTORY, details[i].env_name, \n",
    "                                                                                gamma, reward, '{}_{}'),\n",
    "                                       env.desc, env.colors(), env.directions(),\n",
    "                                       'Value Iteration', 'Step', details[i], only_last=True)\n",
    "            vi_stats.to_csv('{}/VI/{}_{}_{}.csv'.format(OUTPUT_DIRECTORY, details[i].env_name, reward, gamma))\n",
    "    \n",
    "            print('Running PI with reward: {}, gamma: {}'.format(reward, gamma))\n",
    "        \n",
    "            pi = PolicyIteration(env, gamma=gamma, max_conv_steps=1)\n",
    "            pi_stats = run_solver_and_collect(pi)\n",
    "            pi_stats.plot_policies_on_map('{}/images/PI/{}_{}_{}_{}.png'.format(OUTPUT_DIRECTORY, details[i].env_name,\n",
    "                                                                                gamma, reward, '{}_{}'),\n",
    "                                   env.desc, env.colors(), env.directions(),\n",
    "                                   'Policy Iteration', 'Step', details[i], only_last=True)\n",
    "            pi_stats.to_csv('{}/PI/{}_{}_{}.csv'.format(OUTPUT_DIRECTORY, details[i].env_name, reward, gamma))\n",
    "            \n",
    "            for alpha in alphas:\n",
    "                \n",
    "                print('Running Q-Learning with reward: {}, gamma: {}, alpha: {}'.format(reward, gamma, alpha))\n",
    "                \n",
    "                q_learner = QLearning(env, gamma=gamma, alpha=alpha)\n",
    "                q_stats = run_solver_and_collect(q_learner)\n",
    "                q_learner.value[env.terminal_1] = 1\n",
    "                q_learner.value[env.terminal_2] = -1\n",
    "                q_stats.plot_policies_on_map('{}/images/QL/{}_{}_{}_{}_{}_{:.3f}_{:.3f}_{}.png'.format(OUTPUT_DIRECTORY,\n",
    "                                                                                         details[i].env_name, reward,\n",
    "                                                                                         'zeros', gamma, alpha,\n",
    "                                                                                         q_learner.epsilon, q_learner.epsilon_decay,\n",
    "                                                                                         '{}_{}'),\n",
    "                                           env.desc, env.colors(), env.directions(),\n",
    "                                           'Q-Learning', 'Episode', details[i], only_last=True)\n",
    "                q_stats.to_csv('{}/QL/{}_{}_{}_{}_{:.3f}_{:.3f}.csv'.format(OUTPUT_DIRECTORY, details[i].env_name,\n",
    "                                              reward, 'zeros', gamma, alpha, q_learner.epsilon, q_learner.epsilon_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = ValueIteration(envs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_stats = run_solver_and_collect(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(v_stats.vs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-small\n"
     ]
    }
   ],
   "source": [
    "v_stats.plot_policies_on_map('{}/images/VI/{}_{}_{}_{}.png'.format(OUTPUT_DIRECTORY, 'gridworld', value.gamma, -0.04, '{}_{}'),\n",
    "                           value.env.desc, value.env.colors(), value.env.directions(),\n",
    "                           'Value Iteration', 'Step', details[1], only_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyIteration(gridworld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stats = run_solver_and_collect(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stats.plot_policies_on_map('{}/images/PI/{}_{}_{}.png'.format(OUTPUT_DIRECTORY, 'gridworld', policy.gamma, '{}_{}'),\n",
    "                           policy.env.desc, policy.env.colors(), policy.env.directions(),\n",
    "                           'Policy Iteration', 'Step', details, only_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learner = QLearning(GridWorld((16, 16), reward, lg_start, lg_terminal_1, lg_terminal_2, lg_blocked), max_steps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573.0980224609375"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "q_stats = run_solver_and_collect(q_learner)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learner.value[gridworld.terminal_1] = 1\n",
    "q_learner.value[gridworld.terminal_2] = -1\n",
    "q_stats.plot_policies_on_map('{}/images/QL/{}_{}_{}.png'.format(OUTPUT_DIRECTORY, 'gridworld', q_learner.gamma, '{}_{}'),\n",
    "                           q_learner.env.desc, q_learner.env.colors(), q_learner.env.directions(),\n",
    "                           'Q-Learning', 'Step', details, only_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13533122319589064"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learner.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learner.steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
